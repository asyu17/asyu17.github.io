[{"title":"【小白搭建个人博客】——2、Hexo主题简单修改与添加文章","date":"2020-02-06T09:57:01.000Z","path":"posts/5569aaec.html","text":"【小白搭建个人博客】——2、Hexo主题简单修改与添加文章昨天忙活了一天，终于是搞定了域名和hexo博客上传，今天来对官方主题进行一些修改以及添加自己的文章。（昨晚又换了一个主题，目前的主题链接为：https://github.com/yscoder/hexo-theme-indigo）个人博客地址：https://www.asyu17.cn 下篇文章预告：【Python爬虫实例学习篇】——6、获取免费IP代理进阶，在服务器构建一个高可用代理池 目录: 主题修改· 1、修改个人信息· 2、添加站点统计与评论 上传文章 结语 主题修改修改个人信息打开Hexo目录，找到有个theme文件夹，在theme文件夹下选择我们下载的主题文件夹，如：hexo-theme-indigo，打开_config.yml文件进行修改。如： _config.yml 打开后，如图所示： 修改示例 这里是source文件夹下的内容。其他的一些如邮箱的修改等等都大同小异，不一一列举了，下面是添加站点统计和添加评论功能。 source 添加站点统计与评论由于我们在GitHub pages 上传的Hexo源码为静态网页,不能访问数据库，所以我们的站点统计和评论需要借助第三方提供的接口来实现。 （一）、站点统计 首先去友盟注册一个帐号，然后依次选择网站统计-——&gt;立即使用——&gt;添加站点。 友盟 输入完信息后点击添加站点。 t添加信息 在统计代码页面保存id号码 保存id 回到D:\\asyu17_Blog\\Hexo\\themes\\hexo-theme-indigo 目录下的_config.yml文件，添加自己的id号码。 填充空格 （二）、接入评论系统在这里我打算接入Gitment，所以首先需要进入Github的settings页面，如。 进入settings 随后依次点击Developer settings——&gt;OAuth Apps，随后点击Register a new application，如： register 如图，依次填入信息，再点击注册 填信息 得到两个client数据 id 回到D:\\asyu17_Blog\\Hexo\\themes\\hexo-theme-indigo 目录下的_config.yml文件，进行修改。 修改 评论功能效果图： 评论功能 上传文章 参数 功能 post 新建文章 draft 新建草稿 page 新建页面 进入Hexo目录,右键选择Git Base Here，输入 123$ hexo new post &lt;文章的文件名称&gt;如：$ hexo new post 【Python爬虫实例学习篇】——1、获取拉勾网职位信息 效果 创建成功 然后打开所创建的.md文件，进行修改 修改 效果图： 效果图 结语搞了两天，目前就先这样吧~ 再进行更深层次的修改就需要网页开发的基础了，等后面慢慢改造哈哈~ ==微信公众号：== 小术快跑","tags":[{"name":"Github","slug":"Github","permalink":"https://www.asyu17.cn/tags/Github/"},{"name":"Github Pages","slug":"Github-Pages","permalink":"https://www.asyu17.cn/tags/Github-Pages/"},{"name":"Hexo","slug":"Hexo","permalink":"https://www.asyu17.cn/tags/Hexo/"}]},{"title":"【小白搭建个人博客】——1、使用 Github Pages 和 Hexo搭建个人博客","date":"2020-02-05T12:04:01.000Z","path":"posts/1c131fb2.html","text":"【小白搭建个人博客】——1、使用 Github Pages 和 Hexo搭建个人博客一个安静的深夜，一头猿正躺在床上刷着最后的博客。忽然一篇名为 《使用 Github Pages 和 Hexo 搭建自己的独立博客【超级详细的小白教程】》的博客映入眼帘。咦，还有这种骚操作？一个拥有专属博客的梦，又再次浮上心头。好吧，开干！(预告，下篇文章将发布有关文章上传，主题修改等内容) 搭建好的博客地址： https://www.asyu17.cn/ 目录： 前言· 1.什么是Github Pages？· 2.什么是Hexo？ 搭建博客环境· 1.Node.js· 2.Git· 3.Hexo 部署到Git Pages· 1.注册Github帐号· 2.创建代码库· 3.配置SSH· 4.配置个人信息· 5.上传博客 绑定域名· 1.购买域名· 2.绑定域名 个性化设置· 1.更换主题 参考文献 # 前言## 什么是Github Pages？ GitHub Pages 是一个静态网站寄主服务，换言之就是一个静态网页托管的服务。 在使用GitHub Pages时，有以下限制： GitHub Pages 源码仓库限制在1GB大小 发布GibHub Pages 网站最好不要超过1GB GitHub Pages网站有流量限制（每月100GB或者100,000次请求） GitHub Page 网站每个小时限制10次重建 ## 什么是Hexo？ Hexo 是一个快速、简洁且高效的博客框架。Hexo 使用 Markdown（或其他渲染引擎）解析文章，在几秒内，即可利用靓丽的主题生成静态网页。 安装 Hexo 相当简单，只需要先安装下列应用程序即可： Node.js (Node.js 版本需不低于 8.10，建议使用 Node.js 10.0 及以上版本) Git 如果您的电脑中已经安装上述必备程序，那么恭喜您！你可以直接前往 安装 Hexo 步骤。如果您的电脑中尚未安装所需要的程序，请根据以下安装指示完成安装。 # 搭建博客环境## Node.js Node.js 下载地址：https://nodejs.org/zh-cn/小术选择的版本： 12.14.1 长期支持版安装方法：一路默认~。 ## GitGit 下载地址：https://git-scm.com/download/win镜像下载地址：https://npm.taobao.org/mirrors/git-for-windows/小术选择的版本：2.25.0 -64bit安装方法：一路默认~。 ## Hexo在D盘中创建一个Hexo目录,随后进入目录，右键点击Git Base Here，进入MINGW64的页面。 创建Hexo目录 随后输入npm install -g hexo若是长时间无反应可尝试先输入npm config set registry “https://registry.npm.taobao.org&quot;,随后再输入npm install -g hexo 安装hexo 输入 hexo init 进行初始化。(在这里我将目录改为 D:\\asyu17_Blog\\Hexo) 初始化 完成后，依次输入 hexo generate 、hexo server ，启动服务后，可以在 http://localhost:4000/ 查看效果。 启动服务 效果展示 # 部署到Git Pages## 注册Github帐号网址：https://github.com/## 创建代码库进入个人主页后，点击 New 个人主页 创建仓库 创建项目d ## 配置SSH在gitlab，github上面拷贝代码时，通常用到了git clone ssh://XXX命令。其中ssh指secure shell（一种安全的网络协议），git使用这种协议进行远程加密登录。 git使用SSH配置， 初始需要以下三个步骤 使用秘钥生成工具生成rsa秘钥和公钥 将rsa公钥添加到代码托管平台 将rsa秘钥添加到ssh-agent中，为ssh client指定使用的秘钥文件 知道原理后，进行实操。Ctrl+C，停止hexo 服务，输入 ssh-keygen -t rsa -C +你的邮箱地址，如：ssh-keygen -t rsa -C 123@163.com,随后一路按回车即可。 在这里插入图片描述 然后进入Github主页，点击头像，依次点击Settings–&gt;SSH and GPG keys–&gt;SSH keys–&gt;New SSH key 点击Settings 添加密钥： 添加密钥 下面测试连接输入：ssh -T git@github.com 测试1 随后输入：yes，提示如下，则说明连接成功。 提示 ## 配置个人信息Git 会根据用户的名字和邮箱来记录提交，用户名可以自由更改。 12$ git config --global user.name &quot;此处填你的用户名&quot; $ git config --global user.email &quot;此处填你的邮箱&quot; ## 上传博客进入个人博客的仓库，点击 Clone or download 选择 Use SSH use SSH 复制地址后，进入本地博客路径，如D:\\asyu17_Blog\\Hexo,使用文本打开 _config.yml 打开config 随后，拉至最下面，找到如： 寻找 将其修改为如下所示，并保存。 修改config 接下来，在Hexo目录下点击Git Bash Here，依次输入 12$ hexo g$ hexo d 若是提示 d错误 则在输入 npm install hexo-deployer-git –save 后，再输入hexo d，随后再打开自己的项目仓库，会发现博客代码已经上传成功了~ 接下来就可以通过 https://你的用户名.github.io 来访问自己的博客了~，如：https://asyu17.github.io 上传成功 # 绑定域名## 购买域名进入阿里云，输入想要注册的域名 域名 选择一个比较满意的注册！ 注意注册年限一定要结合自己的实际情况！ 域名 ## 绑定域名购买成功后，进入云解析，点击解析设置 进入云解析 点击添加记录 添加记录 解析设置： 解析设置 继续添加第二条：（ip地址在cmd中输入ping asyu17.github.io得到） 解析设置2 添加完成后，等待实名审核完成即可使用了。 # 个性化设置## 更换主题进入主题商店：https://hexo.io/themes/ 选择一个自己喜欢的主题。 主题选择 进入所选主题网站，下拉到最底下，找到基于Hexo的XXX主题搭建，点击XXX，进入Github页面。 例子 如： github页面 复制网址： 复制网址 随后再回到 Hexo 文件夹，进入themes文件夹，右键选择Git Base Here，输入命令： 1$ git clone +网址 如： 1$ git clone https:&#x2F;&#x2F;github.com&#x2F;blinkfox&#x2F;hexo-theme-matery 随后就会在theme目录下生成如下文件夹，hexo-theme-matery 就是我们下载的主题： 下载主题成功 随后用文本打开 _config.yml，找到theme，修改为，theme: hexo-theme-matery，然后关闭并保存。然后在Hexo目录下找到source目录,添加CNAME文件,内容为：www.asyu17.com，注意，不要添加任何后缀（CNAME文件的作用是确保跳转到www，如不添加该文件的话，输入www.asyu17.cn就会跳转到asyu17.cn。若添加该文件，输入asyu17.cn也会跳转到www.asyu17.cn）。 theme设置 在Hexo目录下，右键 Git Base Here，输入以下指令就可在 ：http://localhost:4000 查看部署情况 12$ hexo g$ hexo s 例子 继续输入指令，即可部署到GitHub上： 12$ hexo clean$ hexo g -d 效果图： 效果图 # 参考文献 小术也是第一次使用Github Pages和Hexo搭建自己的博客，在写的过程中一边实践，一边参考了以下几篇文章才最终完成，再次感谢这些大佬的无私奉献，若有侵权请联系我删除。 1.《使用 Github Pages 和 Hexo 搭建自己的独立博客【超级详细的小白教程】》2.《使用hexo+github搭建个人博客(进阶篇)》3.《hexo+github搭建个人博客(超详细教程)》4.《git ssh 配置与原理》 ==微信公众号：== 小术快跑","tags":[{"name":"Github","slug":"Github","permalink":"https://www.asyu17.cn/tags/Github/"},{"name":"Github Pages","slug":"Github-Pages","permalink":"https://www.asyu17.cn/tags/Github-Pages/"},{"name":"Hexo","slug":"Hexo","permalink":"https://www.asyu17.cn/tags/Hexo/"},{"name":"博客","slug":"博客","permalink":"https://www.asyu17.cn/tags/%E5%8D%9A%E5%AE%A2/"}]},{"title":"【自制实用小工具】——1、自制Xpath解析器","date":"2020-02-02T13:57:01.000Z","path":"posts/cceb02f5.html","text":"【自制实用小工具】——1、Xpath解析器由于js脚本的影响，我们请求得到的数据常常与网页显示的数据不一样。而chrome插件xpath helper不能调试本地网页，于是有了制造一个xpath解析器的想法。（粗略尝试了一下，没有问题，大家要是发现bug的话记得评论告诉我啊~）工具： PyQt5 库 Qt designer sys 库 requests 库 lxml 库 步骤：（一）用Qt designer设计界面 界面 （二）将.ui文件转换为.py文件有关（一）、（二）部分的教程可以参考：https://www.jb51.net/article/170810.htm （三）链接按钮将以下代码添加到def setupUi后面 123# 设置按钮控件 self.button_Get_html.clicked.connect(self.Button_Get_Html) self.button_Xpath_Parse.clicked.connect(self.Button_Xpath_Parse) （四）按钮事件以下分别是按钮==Get Html==和按钮 ==Xpath Parse== 的代码： 1234567891011121314151617181920212223242526272829303132333435363738def Button_Get_Html(self): headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36' &#125; url = self.text_Web_Site.toPlainText().strip() if len(url): if url[0] == 'w': url = 'http://' + url session = requests.session() try: res = session.get(url=url, headers=headers, verify=False).content.decode('utf-8','ignore') # 在text_HTML_Code中输出返回内容 self.text_HTML_Code.setPlainText(res) except Exception as e: self.text_HTML_Code.setPlainText(e.__str__()) else: self.text_HTML_Code.setPlainText('网址不能为空！')def Button_Xpath_Parse(self): self.text_Result.document().clear() xpath_syntax=self.text_Xpath_Syntax.toPlainText() html_code=self.text_HTML_Code.toPlainText() html=etree.HTML(html_code) try: results = html.xpath(xpath_syntax) num = 0 for result in results: self.text_Result.append('-'*60+'这里是第 '+str(num)+' 个') # result 有两种格式 try: self.text_Result.append(result.text) except Exception: self.text_Result.append(result) num=num+1 except Exception as e: self.text_Result.setPlainText(e.__str__()) （五）初始化界面123456789101112if __name__ == '__main__': # 每一pyqt5应用程序必须创建一个应用程序对象。sys.argv参数是一个列表，从命令行输入参数。 app = QtWidgets.QApplication(sys.argv) # QWidget部件是pyqt5所有用户界面对象的基类。他为QWidget提供默认构造函数。默认构造函数没有父类。 w = QtWidgets.QWidget() ui = Ui_Asyu17_Xpath_Helper() ui.setupUi(w) w.show() # 系统exit()方法确保应用程序干净的退出 # 的exec_()方法有下划线。因为执行是一个Python关键词。因此，exec_()代替 sys.exit(app.exec_()) 结果展示：测试无问题后，可使用pyinstaller将代码编译成可执行文件~ 结果展示 代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144from PyQt5 import QtCore, QtGui, QtWidgetsimport sysimport requestsfrom lxml import etreerequests.packages.urllib3.disable_warnings()class Ui_Asyu17_Xpath_Helper(object): def setupUi(self, Asyu17_Xpath_Helper): Asyu17_Xpath_Helper.setObjectName(\"Asyu17_Xpath_Helper\") Asyu17_Xpath_Helper.resize(969, 905) self.button_Xpath_Parse = QtWidgets.QPushButton(Asyu17_Xpath_Helper) self.button_Xpath_Parse.setGeometry(QtCore.QRect(830, 860, 75, 31)) self.button_Xpath_Parse.setObjectName(\"button_Xpath_Parse\") self.label = QtWidgets.QLabel(Asyu17_Xpath_Helper) self.label.setGeometry(QtCore.QRect(10, 10, 71, 16)) self.label.setFrameShape(QtWidgets.QFrame.StyledPanel) self.label.setScaledContents(False) self.label.setObjectName(\"label\") self.label_2 = QtWidgets.QLabel(Asyu17_Xpath_Helper) self.label_2.setGeometry(QtCore.QRect(490, 10, 51, 16)) self.label_2.setFrameShape(QtWidgets.QFrame.StyledPanel) self.label_2.setScaledContents(False) self.label_2.setObjectName(\"label_2\") self.label_3 = QtWidgets.QLabel(Asyu17_Xpath_Helper) self.label_3.setGeometry(QtCore.QRect(20, 860, 91, 31)) self.label_3.setObjectName(\"label_3\") self.text_Xpath_Syntax = QtWidgets.QTextBrowser(Asyu17_Xpath_Helper) self.text_Xpath_Syntax.setGeometry(QtCore.QRect(110, 860, 681, 31)) font = QtGui.QFont() font.setFamily(\"Arial\") font.setPointSize(13) self.text_Xpath_Syntax.setFont(font) self.text_Xpath_Syntax.setReadOnly(False) self.text_Xpath_Syntax.setObjectName(\"text_Xpath_Syntax\") self.button_Get_html = QtWidgets.QPushButton(Asyu17_Xpath_Helper) self.button_Get_html.setGeometry(QtCore.QRect(830, 820, 75, 31)) self.button_Get_html.setObjectName(\"button_Get_html\") self.text_Web_Site = QtWidgets.QTextBrowser(Asyu17_Xpath_Helper) self.text_Web_Site.setGeometry(QtCore.QRect(110, 820, 681, 31)) font = QtGui.QFont() font.setFamily(\"Arial\") font.setPointSize(13) self.text_Web_Site.setFont(font) self.text_Web_Site.setReadOnly(False) self.text_Web_Site.setObjectName(\"text_Web_Site\") self.label_4 = QtWidgets.QLabel(Asyu17_Xpath_Helper) self.label_4.setGeometry(QtCore.QRect(20, 820, 91, 31)) self.label_4.setObjectName(\"label_4\") self.layoutWidget = QtWidgets.QWidget(Asyu17_Xpath_Helper) self.layoutWidget.setGeometry(QtCore.QRect(10, 30, 951, 781)) self.layoutWidget.setObjectName(\"layoutWidget\") self.horizontalLayout = QtWidgets.QHBoxLayout(self.layoutWidget) self.horizontalLayout.setContentsMargins(0, 0, 0, 0) self.horizontalLayout.setObjectName(\"horizontalLayout\") self.text_HTML_Code = QtWidgets.QTextBrowser(self.layoutWidget) self.text_HTML_Code.setEnabled(True) font = QtGui.QFont() font.setFamily(\"Arial\") font.setPointSize(12) self.text_HTML_Code.setFont(font) self.text_HTML_Code.setMouseTracking(False) self.text_HTML_Code.setTabletTracking(False) self.text_HTML_Code.setReadOnly(False) self.text_HTML_Code.setObjectName(\"text_HTML_Code\") self.horizontalLayout.addWidget(self.text_HTML_Code) self.text_Result = QtWidgets.QTextBrowser(self.layoutWidget) font = QtGui.QFont() font.setFamily(\"Arial\") font.setPointSize(12) self.text_Result.setFont(font) self.text_Result.setReadOnly(False) self.horizontalLayout.addWidget(self.text_Result) self.retranslateUi(Asyu17_Xpath_Helper) QtCore.QMetaObject.connectSlotsByName(Asyu17_Xpath_Helper) # 设置按钮控件 self.button_Get_html.clicked.connect(self.Button_Get_Html) self.button_Xpath_Parse.clicked.connect(self.Button_Xpath_Parse) def retranslateUi(self, Asyu17_Xpath_Helper): _translate = QtCore.QCoreApplication.translate Asyu17_Xpath_Helper.setWindowTitle(_translate(\"Asyu17_Xpath_Helper\", \"Asyu17 Xpath Helper\")) self.button_Xpath_Parse.setText(_translate(\"Asyu17_Xpath_Helper\", \"Xpath Parse\")) self.label.setText(_translate(\"Asyu17_Xpath_Helper\", \"HTML Code:\")) self.label_2.setText(_translate(\"Asyu17_Xpath_Helper\", \"Result:\")) self.label_3.setText(_translate(\"Asyu17_Xpath_Helper\", \"Xpath Syntax:\")) self.button_Get_html.setText(_translate(\"Asyu17_Xpath_Helper\", \"Get Html\")) self.label_4.setText(_translate(\"Asyu17_Xpath_Helper\", \"Web Site:\")) def Button_Get_Html(self): headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36' &#125; url = self.text_Web_Site.toPlainText().strip() if len(url): if url[0] == 'w': url = 'http://' + url session = requests.session() try: res = session.get(url=url, headers=headers, verify=False).content.decode('utf-8','ignore') # 在text_HTML_Code中输出返回内容 self.text_HTML_Code.setPlainText(res) except Exception as e: self.text_HTML_Code.setPlainText(e.__str__()) else: self.text_HTML_Code.setPlainText('网址不能为空！') def Button_Xpath_Parse(self): self.text_Result.document().clear() xpath_syntax=self.text_Xpath_Syntax.toPlainText() html_code=self.text_HTML_Code.toPlainText() html=etree.HTML(html_code) try: results = html.xpath(xpath_syntax) num = 0 for result in results: self.text_Result.append('-'*60+'这里是第 '+str(num)+' 个') # result 有两种格式 try: self.text_Result.append(result.text) except Exception: self.text_Result.append(result) num=num+1 except Exception as e: self.text_Result.setPlainText(e.__str__())if __name__ == '__main__': # 每一pyqt5应用程序必须创建一个应用程序对象。sys.argv参数是一个列表，从命令行输入参数。 app = QtWidgets.QApplication(sys.argv) # QWidget部件是pyqt5所有用户界面对象的基类。他为QWidget提供默认构造函数。默认构造函数没有父类。 w = QtWidgets.QWidget() ui = Ui_Asyu17_Xpath_Helper() ui.setupUi(w) w.show() # 系统exit()方法确保应用程序干净的退出 # 的exec_()方法有下划线。因为执行是一个Python关键词。因此，exec_()代替 sys.exit(app.exec_()) ==微信公众号：== 小术快跑","tags":[{"name":"Xpath","slug":"Xpath","permalink":"https://www.asyu17.cn/tags/Xpath/"},{"name":"自制","slug":"自制","permalink":"https://www.asyu17.cn/tags/%E8%87%AA%E5%88%B6/"},{"name":"PyQt5","slug":"PyQt5","permalink":"https://www.asyu17.cn/tags/PyQt5/"}]},{"title":"【Python爬虫实例学习篇】——5、【超详细记录】从爬取微博评论数据（免登陆）到生成词云","date":"2020-01-30T12:04:01.000Z","path":"posts/a41631be.html","text":"【Python爬虫实例学习篇】——5、【超详细记录】从爬取微博评论数据（免登陆）到生成词云近段时间新型冠状病毒的问题引起了全国人民的广泛关注，对于这一高传染性的病毒，人们有着不同的声音，而我想通过大数据看看大多数人是怎么想的。 精彩部分提醒：（1）微博评论页详情链接为一个js脚本（2）获取js脚本链接需要该条微博的mid参数（3）获取mid参数需要访问微博主页（4）访问微博主页需要先进行访客认证（5）微博主页几乎是由弹窗构成，所有html代码被隐藏在FM.view()函数的参数中，该参数是json格式 工具： Python 3.6 requests 库 json 库 lxml 库 urllib 库 jieba 库（进行分词） WordCloud 库（产生词云） 目录： 爬取微博评论数据 GetWeiBoRemark.py 生成词云 爬取微博篇论数据 以央视新闻官方微博置顶的第一条微博为例，爬取其评论数据。 央视新闻微博首页 寻找评论页第一步：寻找评论页先用Ctrl+Shift+C 选取评论标签查看其html代码，发现其链接为一个js脚本，那么尝试用fddler看看能不能抓到这个js脚本的包，得到这个js的地址。 评论页地址为一个脚本 找到疑似js包，将数据解码，确认是我们要找的包。 疑似js包 解码与json解析结果 同时，根据“查看更多”可以确定跳转的链接，将这一结果在json解析结果中搜索，可以进一步确定这个js包就是我们要找的包。接下来需要确定这个js包是来源于哪。 找链接 确定链接地址 第二步：找到js包地址js包地址为：“https://weibo.com/aj/v6/comment/small?ajwvr=6&amp;act=list&amp;mid=4465267293291962&amp;uid=3655689037&amp;isMain=true&amp;dissDataFromFeed=%5Bobject%20Object%5D&amp;ouid=2656274875&amp;location=page_100206_home&amp;comment_type=0&amp;_t=0&amp;__rnd=1580130440282”，链接很长，且参数很多，根据以往经验，我们尝试删除一些参数进行访问测试。经过测试发现只需==mid==这一个参数即可获取该数据包。所以有效js包地址为：”https://weibo.com/aj/v6/comment/small?mid=4465267293291962“。 寻找有效js地址 那么，接下来的工作就是去寻找mid这以参数的值（猜测应该是微博的唯一序列号）。在Fiddler中搜索 “mid=4465267293291962”可以发现在央视新闻首页中，每条微博里面都包含了该微博的mid信息。 找到mid 用Ctrl+Shift+C 任意选取一条微博，可以发现有一个 “mid” 属性，里面包含mid的数据 获取mid 用XPath Helper进行调试，没有问题，接下来在python上实现这部分代码，==（经过后面测试发现获取评论数据只需获取微博的mid即可，因此下面这几步可以跳过，但是为例保证探索过程的完整性，我将其留在了这里）== xpath调试 得到评论页的代码： 123456789101112131415import requestsfrom lxml import etreerequests.packages.urllib3.disable_warnings()name=\"cctvxinwen\"home_url='https://weibo.com/'+nameheaders=&#123; \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36\"&#125;session=requests.session()res=session.get(home_url,headers=headers,timeout=20,verify=False)html=etree.HTML(res.content.decode('gb2312',\"ignore\"))mid_list=html.xpath(\"//div/@mid\") 本以为可以轻松获取新闻主页，结果发现若是没有附带cookie的话，会自动跳转到微博登陆验证页面。 微博验证页面 用chrome的无痕访问结合fiddler，重新抓包，可以清晰的发现微博的验证登陆流程。其中最重要的是第11号包，其余6号包用于提供参数解析，9号包提供==参数s==和参数==sp==的数据，8号包用于提供9号包访问链接中参数的数据。 微博验证流程 对于我们的爬虫来说，只需要从第8号包开始访问即可，8号包需要提交的数据为(常量)： 1cb=gen_callback&amp;fp=&#123;\"os\":\"1\",\"browser\":\"Chrome80,0,3970,5\",\"fonts\":\"undefined\",\"screenInfo\":\"1920*1080*24\",\"plugins\":\"Portable Document Format::internal-pdf-viewer::Chrome PDF Plugin|::mhjfbmdgcfjbbpaeojofohoefgiehjai::Chrome PDF Viewer|::internal-nacl-plugin::Native Client\"&#125; 需要注意的是，在使用requests库时，需要向协议头中添加：==’Content-Type’: ‘application/x-www-form-urlencoded’==，否则返回数据为空。 9号包的链接为： “https://passport.weibo.com/visitor/visitor?a=incarnate&amp;t=39dNzddUHqqZOWZQbMiDrOvkea/y7s06WFyX%2BzsGk8w%3D&amp;w=2&amp;c=095&amp;gc=&amp;cb=cross_domain&amp;from=weibo&amp;_rand=0.5282100349168277”，其中_rand参数可以忽略。这里需要注意，添加tid参数时，tid参数需要 url编码。 在完成9号包的访问后，就可以获取央视新闻微博的主页了： 央视新闻微博主页 第三步：获取评论页链接在这里有一个很有意思的现象，当我在网页用xpath helper调试的时候，能够非常容易的获取对应的属性值，但是一旦将该xpath语法应用与python中进行解析时，总是得到空的数据。经过一番调试发现，是由于微博这个 ==“欧盟隐私弹窗”==所致。所有我们需要的数据全部被隐藏在这个弹窗之中，微博页面的所有内容通过调用 FM.view() 这个函数显示出来，网页的html代码就隐藏在 FM.view() 函数的json格式的参数中。 寻找评论链接 解析FM.view()的参数 第一部分的代码为： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950import requestsimport jsonfrom urllib import parsefrom lxml import etreerequests.packages.urllib3.disable_warnings()session = requests.session()session.verify = Falsesession.timeout = 20name = \"cctvxinwen\"home_url = 'https://weibo.com/' + nameurl1 = \"https://passport.weibo.com/visitor/genvisitor\"urljs='https://weibo.com/aj/v6/comment/small?mid='headers = &#123; \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36\"&#125;data = \"cb=gen_callback&amp;fp=%7B%22os%22%3A%221%22%2C%22browser%22%3A%22Chrome80%2C0%2C3970%2C5%22%2C%22fonts%22%3A%22undefined%22%2C%22screenInfo%22%3A%221920*1080*24%22%2C%22plugins%22%3A%22Portable%20Document%20Format%3A%3Ainternal-pdf-viewer%3A%3AChrome%20PDF%20Plugin%7C%3A%3Amhjfbmdgcfjbbpaeojofohoefgiehjai%3A%3AChrome%20PDF%20Viewer%7C%3A%3Ainternal-nacl-plugin%3A%3ANative%20Client%22%7D\"# 获取tidheaders.update(&#123;'Content-Type': 'application/x-www-form-urlencoded'&#125;)tid = json.loads(session.post(url=url1, headers=headers, data=data).content.decode('utf-8')[36:-2])['data']['tid']del headers['Content-Type']# 获取访客cookieurl2 = \"https://passport.weibo.com/visitor/visitor?a=incarnate&amp;t=\" + parse.quote( tid) + \"&amp;w=2&amp;c=095&amp;gc=&amp;cb=cross_domain&amp;from=weibo\"session.get(url=url2.encode('utf-8'), headers=headers)# 访问微博主页，解析获取评论页面res = session.get(url=home_url, headers=headers)html = etree.HTML(res.content.decode('utf-8', \"ignore\"))# 含有mid的html代码被隐藏在这一个json中mid_json = json.loads(html.xpath(\"//script\")[38].text[8:-1])mid_html=etree.HTML(mid_json['html'])mids=mid_html.xpath(\"//div/@mid\")# 获取第一条微博的js包地址urljs=urljs+str(mids[0])res=session.get(url=urljs, headers=headers)js_json_html=json.loads(res.content)['data']['html']print(js_json_html)print(\"该微博当前评论数为：\"+str(json.loads(res.content)['data']['count']))# 解析获取评论页地址js_html=etree.HTML(js_json_html)url_remark=js_html.xpath(\"//a[@target='_blank']/@href\")[-1]url_remark=\"https:\"+url_remark 获取并评论获取评论页后，我们非常容易的就能找到评论数据的json包，如图所示： 评论json包 该json包的链接为：“https://weibo.com/aj/v6/comment/big?ajwvr=6&amp;id=4466098865136889&amp;root_comment_max_id=206048831614549&amp;root_comment_max_id_type=0&amp;root_comment_ext_param=&amp;page=4&amp;filter=hot&amp;sum_comment_number=13481&amp;filter_tips_before=1&amp;from=singleWeiBo&amp;__rnd=1580364082863”==其中，有效json包链接为：==“https://weibo.com/aj/v6/comment/big?ajwvr=6&amp;id=4466098865136889&amp;page=1&amp;from=singleWeiBo”。获取json包之后，既可以提取出评论数据，其代码如下 ==(由于该json包不需要其他参数，需要额外提供mid和sum_comment_number参数即可，因此我们在获取mid后可以直接跳到这一步)== ：，经过测试，若不提供sum_comment_number参数只能提取前几页的微博评论。爬取评论如下： 评论结果 GetWeiBoRemark.py 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113import requestsimport jsonfrom urllib import parsefrom lxml import etree# 预设requests.packages.urllib3.disable_warnings()session = requests.session()session.verify = Falsesession.timeout = 20headers = &#123; \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36\"&#125;def getweiboremark(name, index=1, num=-1): # name参数代表微博主页的名称 # index参数代表目标微博的序号 # num代表要爬的评论数，-1代表所有评论 # 返回结果为评论列表 home_url = 'https://weibo.com/' + name tid = get_tid() get_cookie(tid=tid) mids = get_mids(home_url=home_url) remark_data = get_remarkdata(num=num, mids=mids, index=index) return remark_datadef get_tid(): # 获取tid url1 = \"https://passport.weibo.com/visitor/genvisitor\" data = \"cb=gen_callback&amp;fp=%7B%22os%22%3A%221%22%2C%22browser%22%3A%22Chrome80%2C0%2C3970%2C5%22%2C%22fonts%22%3A%22undefined%22%2C%22screenInfo%22%3A%221920*1080*24%22%2C%22plugins%22%3A%22Portable%20Document%20Format%3A%3Ainternal-pdf-viewer%3A%3AChrome%20PDF%20Plugin%7C%3A%3Amhjfbmdgcfjbbpaeojofohoefgiehjai%3A%3AChrome%20PDF%20Viewer%7C%3A%3Ainternal-nacl-plugin%3A%3ANative%20Client%22%7D\" headers.update(&#123;'Content-Type': 'application/x-www-form-urlencoded'&#125;) res=session.post(url=url1, headers=headers, data=data).content.decode('utf-8')[36:-2] tid = json.loads(res)['data']['tid'] del headers['Content-Type'] return tiddef get_cookie(tid, session=session): # 获取访客cookie url2 = \"https://passport.weibo.com/visitor/visitor?a=incarnate&amp;t=\" + parse.quote( tid) + \"&amp;w=2&amp;c=095&amp;gc=&amp;cb=cross_domain&amp;from=weibo\" session.get(url=url2.encode('utf-8'), headers=headers)def get_mids(home_url, session=session,try_num=0): # 访问微博主页，解析获取评论页面 # 要想获取mids，必须先获取cookie res = session.get(url=home_url, headers=headers).content.decode('utf-8', \"ignore\") html = etree.HTML(res) try: # 含有mid的html代码被隐藏在这一个json中 # 38是企业微博 mid_json = json.loads(html.xpath(\"//script\")[38].text[8:-1]) mid_html = etree.HTML(mid_json['html']) mids = mid_html.xpath(\"//@mid\") mids[0] except Exception as e: print(e) try: # 32是个人微博 mid_json = json.loads(html.xpath(\"//script\")[32].text[8:-1]) mid_html = etree.HTML(mid_json['html']) mids = mid_html.xpath(\"//@mid\") mids[0] except Exception as e: print(e) if try_num&lt;2: mids=get_mids(home_url, session=session, try_num=try_num+1) if len(mids)==0: print(\"多次获取mid失败！程序暂停运行！\") quit() return midsdef get_remarkdata(num, mids, index=1): # 获取评论数据 url_remarkdata = 'https://weibo.com/aj/v6/comment/big?ajwvr=6&amp;id=&#123;mid&#125;&amp;page=&#123;page&#125;&amp;sum_comment_number=&#123;comment_number&#125;&amp;from=singleWeiBo' page = 1 remark_data_new = [] current_num = 0 while True: print(\"-\" * 50) url_remarkdata_new = url_remarkdata.format(mid=str(mids[index]),comment_number=str(current_num),page=str(page)) page = page + 1 print(\"正在采集第 \" + str(page - 1) + \" 页评论！\") res = session.get(url=url_remarkdata_new, headers=headers) remark_html = etree.HTML(json.loads(res.content.decode(encoding='utf-8'))['data']['html']) remark_data = remark_html.xpath(\"//div[@class='list_con']/div[1]/text()\") remark_num = json.loads(res.content.decode(encoding='utf-8'))['data']['count'] if page == 2: print(\"本条微博共有 \"+str(remark_num)+' 个评论！') if num == -1: num = remark_num elif num &gt; remark_num: num = remark_num for i in remark_data: if i[0:1] == '：': i = i[1:] remark_data_new.append(i.strip()) current_num = len(remark_data_new) print(\"当前已采集：\" + str(current_num) + \" 个评论，剩余：\" + str(num - current_num) + \"个待采集！\") if (num &lt;= current_num): break return remark_data_newdef save_remarkdata(name,data): with open(name,'w',encoding='utf-8') as fp: fp.write(data) fp.flush() fp.close() 结果展示： 结果展示1 3、生成词云代码如下： 1234567891011121314151617181920from wordcloud import WordCloudimport matplotlib.pyplot as pltimport jiebaimport GetWeiBoRemarkdef producewordcloud(data,mask=None): word_cloud=WordCloud(background_color=\"white\",font_path='msyh.ttc',mask=mask,max_words=200,max_font_size=100,width=1000,height=860).generate(' '.join(jieba.cut(data,cut_all=False))) plt.figure() plt.imshow(word_cloud, interpolation='bilinear') plt.axis(\"off\") # 不显示坐标轴 plt.show()if __name__ == '__main__': # cctvxinwen remark_data=GetWeiBoRemark.getweiboremark(name=\"cctvxinwen\",index=2,num=100) str_remark_data='' for i in remark_data: str_remark_data=str_remark_data+str(i) GetWeiBoRemark.save_remarkdata(name='cctvxinwen.txt',data=str_remark_data) producewordcloud(str_remark_data) ==目标微博== 目标微博 ==下面是爬3000条数据做出的词云：== 词云 爬数据 ==微信公众号：== 小术快跑","tags":[{"name":"Python","slug":"Python","permalink":"https://www.asyu17.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://www.asyu17.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"bilibili","slug":"bilibili","permalink":"https://www.asyu17.cn/tags/bilibili/"}]},{"title":"【Python爬虫实例学习篇】——4、超详细爬取bilibili视频","date":"2020-01-21T10:54:01.000Z","path":"posts/442b3e6b.html","text":"【Python爬虫实例学习篇】——4、超详细爬取bilibili视频由于经常在B站上学习，但无奈于家里网络太差，在线观看卡顿严重，于是萌生了下载视频的想法（如果只是单纯想下载视频，请用you-get库）。废话不多说直接开干。（我发现好像很多人在爬bilibili视频的时候都有用到某个API然后还需要一个cid参数，这些在本文中没有用到。。。。）（另外再说明一下，第3篇文章没有通过审核，要看的话去公众号哈哈）使用工具 python3.6 requests库 lxml库（xpath解析） json库（解析json数据获取下载链接） ffmpeg（合并视频和音频） 目录 确定视频资源地址 下载测试 下载视频和音频（两种方法） 合并视频和音频 BiliBiliVideo.py 运行结果 确定视频资源地址 (1) 用Chrome随便打开一个视频，==Ctrl+Shift+C==选择视频框尝试获取视频的链接。结果发现获取的链接地址为：blob:https://www.bilibili.com/198785ae-c0e6-48c1-b27b-36c5af8935c6 ，这是一个blob加密的链接，不能直接访问。 获取视频blob链接 (2) 网上查找资料后，这篇文章给了我灵感，思路：对网页抓包，抓取到视频分片的链接，再利用所抓到的链接信息进行定位。 Fiddler抓包获取视频分片链接 (3) 可以定位到所有视频分片的信息全部来源于 https://www.bilibili.com/video/av56643958 定位到地址 (4) 对这部分json代码进行解析（完整json数据太大，请自行去 B站 找到对应位置观看），可以发现：&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;==quality==参数是指视频清晰度，112为高清1080p+、80为高清1080p、64为高清、32为清晰、16为流畅。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;==duration==参数是指视频长度，单位为秒。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;==frameRate==参数为帧率。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;==SegmentBase==参数应该是视频片初始片大小和片基址范围，单位为字节。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;==deadline==参数是在url里的参数，指示了链接失效的时间戳。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100&#123; \"code\": 0, \"message\": \"0\", \"ttl\": 1, \"data\": &#123; \"from\": \"local\", \"result\": \"suee\", \"message\": \"\", \"quality\": 64, \"format\": \"flv720\", \"timelength\": 1504366, \"accept_format\": \"flv720,flv480,flv360\", \"accept_description\": [ \"高清 720P\", \"清晰 480P\", \"流畅 360P\" ], \"accept_quality\": [ 64, 32, 16 ], \"video_codecid\": 7, \"seek_param\": \"start\", \"seek_type\": \"offset\", \"dash\": &#123; \"duration\": 1505, \"minBufferTime\": 1.5, \"min_buffer_time\": 1.5, \"video\": [ &#123; \"id\": 64, \"baseUrl\": \"http://upos-sz-mirrorkodo.bilivideo.com/upgcxcode/03/88/98958803/98958803-1-30064.m4s?e=ig8euxZM2rNcNbdlhoNvNC8BqJIzNbfqXBvEqxTEto8BTrNvN0GvT90W5JZMkX_YN0MvXg8gNEV4NC8xNEV4N03eN0B5tZlqNxTEto8BTrNvNeZVuJ10Kj_g2UB02J0mN0B5tZlqNCNEto8BTrNvNC7MTX502C8f2jmMQJ6mqF2fka1mqx6gqj0eN0B599M=&amp;uipk=5&amp;nbs=1&amp;deadline=1579449043&amp;gen=playurl&amp;os=kodobv&amp;oi=1971869914&amp;trid=9412dee30c4640c6907ef910ea2cb04cu&amp;platform=pc&amp;upsig=4b952dd652c9922b546b99e44756fe0a&amp;uparams=e,uipk,nbs,deadline,gen,os,oi,trid,platform&amp;mid=352741151\", \"base_url\": \"http://upos-sz-mirrorkodo.bilivideo.com/upgcxcode/03/88/98958803/98958803-1-30064.m4s?e=ig8euxZM2rNcNbdlhoNvNC8BqJIzNbfqXBvEqxTEto8BTrNvN0GvT90W5JZMkX_YN0MvXg8gNEV4NC8xNEV4N03eN0B5tZlqNxTEto8BTrNvNeZVuJ10Kj_g2UB02J0mN0B5tZlqNCNEto8BTrNvNC7MTX502C8f2jmMQJ6mqF2fka1mqx6gqj0eN0B599M=&amp;uipk=5&amp;nbs=1&amp;deadline=1579449043&amp;gen=playurl&amp;os=kodobv&amp;oi=1971869914&amp;trid=9412dee30c4640c6907ef910ea2cb04cu&amp;platform=pc&amp;upsig=4b952dd652c9922b546b99e44756fe0a&amp;uparams=e,uipk,nbs,deadline,gen,os,oi,trid,platform&amp;mid=352741151\", \"backupUrl\": [ \"http://upos-sz-mirrorks3.bilivideo.com/upgcxcode/03/88/98958803/98958803-1-30064.m4s?e=ig8euxZM2rNcNbdlhoNvNC8BqJIzNbfqXBvEqxTEto8BTrNvN0GvT90W5JZMkX_YN0MvXg8gNEV4NC8xNEV4N03eN0B5tZlqNxTEto8BTrNvNeZVuJ10Kj_g2UB02J0mN0B5tZlqNCNEto8BTrNvNC7MTX502C8f2jmMQJ6mqF2fka1mqx6gqj0eN0B599M=&amp;uipk=5&amp;nbs=1&amp;deadline=1579449043&amp;gen=playurl&amp;os=ks3bv&amp;oi=1971869914&amp;trid=9412dee30c4640c6907ef910ea2cb04cu&amp;platform=pc&amp;upsig=62448ee8270504e8e729d25fc402dc7f&amp;uparams=e,uipk,nbs,deadline,gen,os,oi,trid,platform&amp;mid=352741151\" ], \"backup_url\": [ \"http://upos-sz-mirrorks3.bilivideo.com/upgcxcode/03/88/98958803/98958803-1-30064.m4s?e=ig8euxZM2rNcNbdlhoNvNC8BqJIzNbfqXBvEqxTEto8BTrNvN0GvT90W5JZMkX_YN0MvXg8gNEV4NC8xNEV4N03eN0B5tZlqNxTEto8BTrNvNeZVuJ10Kj_g2UB02J0mN0B5tZlqNCNEto8BTrNvNC7MTX502C8f2jmMQJ6mqF2fka1mqx6gqj0eN0B599M=&amp;uipk=5&amp;nbs=1&amp;deadline=1579449043&amp;gen=playurl&amp;os=ks3bv&amp;oi=1971869914&amp;trid=9412dee30c4640c6907ef910ea2cb04cu&amp;platform=pc&amp;upsig=62448ee8270504e8e729d25fc402dc7f&amp;uparams=e,uipk,nbs,deadline,gen,os,oi,trid,platform&amp;mid=352741151\" ], \"bandwidth\": 359889, \"mimeType\": \"video/mp4\", \"mime_type\": \"video/mp4\", \"codecs\": \"avc1.64001F\", \"width\": 960, \"height\": 534, \"frameRate\": \"25\", \"frame_rate\": \"25\", \"sar\": \"801:800\", \"startWithSap\": 1, \"start_with_sap\": 1, \"SegmentBase\": &#123; \"Initialization\": \"0-995\", \"indexRange\": \"996-4639\" &#125;, \"segment_base\": &#123; \"initialization\": \"0-995\", \"index_range\": \"996-4639\" &#125;, \"codecid\": 7 &#125;, // 此处省略部分数据 &#123; \"id\": 30216, \"baseUrl\": \"http://upos-hz-mirrorks3u.acgvideo.com/upgcxcode/03/88/98958803/98958803-1-30216.m4s?e=ig8euxZM2rNcNbdlhoNvNC8BqJIzNbfqXBvEqxTEto8BTrNvN0GvT90W5JZMkX_YN0MvXg8gNEV4NC8xNEV4N03eN0B5tZlqNxTEto8BTrNvNeZVuJ10Kj_g2UB02J0mN0B5tZlqNCNEto8BTrNvNC7MTX502C8f2jmMQJ6mqF2fka1mqx6gqj0eN0B599M=&amp;uipk=5&amp;nbs=1&amp;deadline=1579449043&amp;gen=playurl&amp;os=ks3u&amp;oi=1971869914&amp;trid=9412dee30c4640c6907ef910ea2cb04cu&amp;platform=pc&amp;upsig=669eccff96c56f5586d174870a496b12&amp;uparams=e,uipk,nbs,deadline,gen,os,oi,trid,platform&amp;mid=352741151\", \"base_url\": \"http://upos-hz-mirrorks3u.acgvideo.com/upgcxcode/03/88/98958803/98958803-1-30216.m4s?e=ig8euxZM2rNcNbdlhoNvNC8BqJIzNbfqXBvEqxTEto8BTrNvN0GvT90W5JZMkX_YN0MvXg8gNEV4NC8xNEV4N03eN0B5tZlqNxTEto8BTrNvNeZVuJ10Kj_g2UB02J0mN0B5tZlqNCNEto8BTrNvNC7MTX502C8f2jmMQJ6mqF2fka1mqx6gqj0eN0B599M=&amp;uipk=5&amp;nbs=1&amp;deadline=1579449043&amp;gen=playurl&amp;os=ks3u&amp;oi=1971869914&amp;trid=9412dee30c4640c6907ef910ea2cb04cu&amp;platform=pc&amp;upsig=669eccff96c56f5586d174870a496b12&amp;uparams=e,uipk,nbs,deadline,gen,os,oi,trid,platform&amp;mid=352741151\", \"backupUrl\": [ \"http://upos-sz-mirrorks3.bilivideo.com/upgcxcode/03/88/98958803/98958803-1-30216.m4s?e=ig8euxZM2rNcNbdlhoNvNC8BqJIzNbfqXBvEqxTEto8BTrNvN0GvT90W5JZMkX_YN0MvXg8gNEV4NC8xNEV4N03eN0B5tZlqNxTEto8BTrNvNeZVuJ10Kj_g2UB02J0mN0B5tZlqNCNEto8BTrNvNC7MTX502C8f2jmMQJ6mqF2fka1mqx6gqj0eN0B599M=&amp;uipk=5&amp;nbs=1&amp;deadline=1579449043&amp;gen=playurl&amp;os=ks3bv&amp;oi=1971869914&amp;trid=9412dee30c4640c6907ef910ea2cb04cu&amp;platform=pc&amp;upsig=b6d7b3957f5dbcbf686f267851ec42dd&amp;uparams=e,uipk,nbs,deadline,gen,os,oi,trid,platform&amp;mid=352741151\" ], \"backup_url\": [ \"http://upos-sz-mirrorks3.bilivideo.com/upgcxcode/03/88/98958803/98958803-1-30216.m4s?e=ig8euxZM2rNcNbdlhoNvNC8BqJIzNbfqXBvEqxTEto8BTrNvN0GvT90W5JZMkX_YN0MvXg8gNEV4NC8xNEV4N03eN0B5tZlqNxTEto8BTrNvNeZVuJ10Kj_g2UB02J0mN0B5tZlqNCNEto8BTrNvNC7MTX502C8f2jmMQJ6mqF2fka1mqx6gqj0eN0B599M=&amp;uipk=5&amp;nbs=1&amp;deadline=1579449043&amp;gen=playurl&amp;os=ks3bv&amp;oi=1971869914&amp;trid=9412dee30c4640c6907ef910ea2cb04cu&amp;platform=pc&amp;upsig=b6d7b3957f5dbcbf686f267851ec42dd&amp;uparams=e,uipk,nbs,deadline,gen,os,oi,trid,platform&amp;mid=352741151\" ], \"bandwidth\": 67100, \"mimeType\": \"audio/mp4\", \"mime_type\": \"audio/mp4\", \"codecs\": \"mp4a.40.2\", \"width\": 0, \"height\": 0, \"frameRate\": \"\", \"frame_rate\": \"\", \"sar\": \"\", \"startWithSap\": 0, \"start_with_sap\": 0, \"SegmentBase\": &#123; \"Initialization\": \"0-907\", \"indexRange\": \"908-4551\" &#125;, \"segment_base\": &#123; \"initialization\": \"0-907\", \"index_range\": \"908-4551\" &#125;, \"codecid\": 0 &#125; ] &#125; &#125;, \"session\": \"da9c24388db43b3dfe81ebd676d5e41b\", \"videoFrame\": &#123; &#125;&#125;&#125; 下载测试(1) 既然确定了上述链接就是我们要请求的视频链接，那么尝试直接发送获取请求看看。&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;结果。。。。403错误，服务器拒绝访问。 403错误 (2) 继续回到Fiddler检查抓包数据发现，同一链接反复出现。仔细观察返回码发现：返回码为200时无数据、请求方式为OPTIONS，返回码为206时有数据、请求方式为GET。查阅资料后猜测，在获取b站视频之前需要用OPTION方式向请求服务器分配资源，然后再用GET方式获取视频分片。 返回码不同 (3) 下面进行获取一个视频分片的测试，考虑到不管是这个OPTIONS请求还是GET请求，其connect属性皆是kepp-alive，考虑使用requests.session()来保持会话。 1234567891011121314151617181920212223242526272829303132333435363738394041import requests# url1 为视频链接、url2为音频链接url='https://cn-hbwh2-cmcc-bcache-07.bilivideo.com/upgcxcode/03/88/98958803/98958803-1-30064.m4s?e=ig8euxZM2rNcNbdlhoNvNC8BqJIzNbfqXBvEqxTEto8BTrNvN0GvT90W5JZMkX_YN0MvXg8gNEV4NC8xNEV4N03eN0B5tZlqNxTEto8BTrNvNeZVuJ10Kj_g2UB02J0mN0B5tZlqNCNEto8BTrNvNC7MTX502C8f2jmMQJ6mqF2fka1mqx6gqj0eN0B599M=&amp;uipk=5&amp;nbs=1&amp;deadline=1579518843&amp;gen=playurl&amp;os=bcache&amp;oi=1971869869&amp;trid=0cfd59d728114a54bd4747a01f87c9bbu&amp;platform=pc&amp;upsig=2a9e83f3258a7e2694bc83a5fbab8664&amp;uparams=e,uipk,nbs,deadline,gen,os,oi,trid,platform&amp;mid=352741151&amp;origin_cdn=ks3'url2='http://upos-hz-mirrorks3u.acgvideo.com/upgcxcode/03/88/98958803/98958803-1-30216.m4s?e=ig8euxZM2rNcNbdlhoNvNC8BqJIzNbfqXBvEqxTEto8BTrNvN0GvT90W5JZMkX_YN0MvXg8gNEV4NC8xNEV4N03eN0B5tZlqNxTEto8BTrNvNeZVuJ10Kj_g2UB02J0mN0B5tZlqNCNEto8BTrNvNC7MTX502C8f2jmMQJ6mqF2fka1mqx6gqj0eN0B599M=&amp;uipk=5&amp;nbs=1&amp;deadline=1579519469&amp;gen=playurl&amp;os=ks3u&amp;oi=1971869869&amp;trid=99ee525d6c7f4bc8a414a537797e31f3u&amp;platform=pc&amp;upsig=ce817a7120709c60ac43cf095de05c8d&amp;uparams=e,uipk,nbs,deadline,gen,os,oi,trid,platform&amp;mid=352741151'headers1=&#123; 'Host': 'cn-hbwh2-cmcc-bcache-04.bilivideo.com', 'Connection': 'keep-alive', 'Access-Control-Request-Method': 'GET', 'Origin': 'https://www.bilibili.com', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36', 'Access-Control-Request-Headers': 'range', 'Accept': '*/*', 'Sec-Fetch-Site': 'cross-site', 'Sec-Fetch-Mode': 'cors', 'Referer': 'https://www.bilibili.com/video/av56643958?t=262', 'Accept-Encoding': 'gzip, deflate, br', 'Accept-Language': 'zh-CN,zh;q=0.9'&#125;headers2=&#123; 'Host': 'cn-hbwh2-cmcc-bcache-04.bilivideo.com', 'Connection': 'keep-alive', 'Origin': 'https://www.bilibili.com', 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36', 'Accept': '*/*', 'Sec-Fetch-Site': 'cross-site', 'Sec-Fetch-Mode': 'cors', 'Referer': 'https://www.bilibili.com/video/av56643958?t=262', 'Accept-Encoding': 'identity', 'Accept-Language': 'zh-CN,zh;q=0.9', 'Range': 'bytes=0-907'&#125;session=requests.session()session.options(url=url1,headers=headers1)res=session.get(url=url1,headers=headers2)print(res)with open('test1.mp4','wb') as fp: fp.write(res.content) fp.flush() fp.close() 发现是能够成功下载视频的，但是该视频不能打开。 下载成功 (4) 重新设置Range的范围为’Range’: ‘bytes=0-4639000’ 后，视频大小为4.42MB，能够正常观看时长为59秒（视频无声音）。猜测：之前下载的908字节不足以构成一个（该视频分辨率为960*534，故猜测应该是62.57kb构成一个画面），另外视频无声音，查找资料后得知，B站的视频和音频是分离的（此时联想到json解析最后一组数据没有分辨率，猜测可能是音频的链接）。 正常观看 验证成功，最后一个链接为音频的链接 音频文件 3、下载视频和音频（两种方法）方法1：取消range参数直接一次下载整个视频或音频 整段下载 方法2：利用416报错码进行分片下载，每次下载1MB的资源，最后一次将range设置为，’Range’: ‘bytes=上一次末尾-‘。从而实现分片下载。 416报错 代码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354import requestsimport jsonfrom lxml import etree# 防止因https证书问题报错requests.packages.urllib3.disable_warnings()headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36', 'Referer': 'https://www.bilibili.com/'&#125;def GetBiliVideo(homeurl,session=requests.session()): res = session.get(url=homeurl, headers=headers, verify=False) html = etree.HTML(res.content) videoinforms = str(html.xpath('//head/script[3]/text()')[0])[20:] videojson = json.loads(videoinforms) # 获取视频链接和音频链接 VideoURL = videojson['data']['dash']['video'][0]['baseUrl'] AudioURl = videojson['data']['dash']['audio'][0]['baseUrl'] print(videojson) #获取视频资源的名称 name = str(html.xpath(\"//h1/@title\")[0].encode('ISO-8859-1').decode('utf-8')) # 下载视频和音频 BiliBiliDownload(url=VideoURL, name=name + '_Video', session=session) BiliBiliDownload(homeurl,url=AudioURl, name=name + '_Audio', session=session)def BiliBiliDownload(homeurl,url, name, session=requests.session()): headers.update(&#123;'Referer': homeurl&#125;) session.options(url=url, headers=headers,verify=False) # 每次下载1M的数据 begin = 0 end = 1024*512-1 flag=0 while True: headers.update(&#123;'Range': 'bytes='+str(begin) + '-' + str(end)&#125;) res = session.get(url=url, headers=headers,verify=False) if res.status_code != 416: begin = end + 1 end = end + 1024*512 else: headers.update(&#123;'Range': str(end + 1) + '-'&#125;) res = session.get(url=url, headers=headers,verify=False) flag=1 with open(name + '.mp4', 'ab') as fp: fp.write(res.content) fp.flush() # data=data+res.content if flag==1: fp.close() break 4、合并视频和音频合并视频和音频我查阅了很多资料，最终决定使用ffmpeg来完成这一操作。z在用合并之前，需要先去安装ffmpeg，详情请参考这篇文章 ffmpeg安装。若是运行过程中出现ffmpeg+一堆乱码,可以参考这篇文章。下面是合并音频代码： 需要先在ffmpeg库的video方法中添加如下代码：12345678910111213# 组合音频和视频 （自己加的）def combine_audio(video_file, audiio_file, out_file): try: cmd ='D:/python/ffmpeg-20200115-0dc0837-win64-static/bin/ffmpeg -i '+video_file+' -i '+audiio_file+' -acodec copy '+out_file print(cmd) subprocess.call(cmd, shell=True) # \"Muxing Done print('Muxing Done') if res != 0: return False return True except Exception: return False 随后在自己的BiliBiliVideo.py中调用如下代码即可123# 一下path都需要使用全路径def CombineVideoAudio(videopath,audiopath,outpath): ffmpeg.video.combine_audio(videopath,audiopath,outpath) BiliBiliVideo.py以下是完整代码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100import requestsimport jsonfrom lxml import etreeimport ffmpeg.videoimport osrequests.packages.urllib3.disable_warnings()headers = &#123; 'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36', 'Referer': 'https://www.bilibili.com/'&#125;def GetBiliVideo(homeurl,num,session=requests.session()): res = session.get(url=homeurl, headers=headers, verify=False) html = etree.HTML(res.content) videoinforms = str(html.xpath('//head/script[3]/text()')[0])[20:] videojson = json.loads(videoinforms) # 获取详情信息列表 #listinform = str(html.xpath('//head/script[4]/text()')[0]) listinform = str(html.xpath('//head/script[4]/text()')[0].encode('ISO-8859-1').decode('utf-8'))[25:-122] listjson=json.loads(listinform) # 获取视频链接和音频链接 try: # 2018年以后的b站视频，音频和视频分离 VideoURL = videojson['data']['dash']['video'][0]['baseUrl'] AudioURl = videojson['data']['dash']['audio'][0]['baseUrl'] flag=0 except Exception: # 2018年以前的b站视频，格式为flv VideoURL = videojson['data']['durl'][0]['url'] flag=1 # 获取文件夹的名称 dirname = str(html.xpath(\"//h1/@title\")[0].encode('ISO-8859-1').decode('utf-8')) if not os.path.exists(dirname): # 如果不存在则创建目录 # 创建目录操作函数 os.makedirs(dirname) print('目录文件创建成功!') # 获取每一集的名称 name=listjson['videoData']['pages'][num]['part'] print(name) # 下载视频和音频 print('正在下载 \"'+name+'\" 的视频····') BiliBiliDownload(homeurl=homeurl,url=VideoURL, name=os.getcwd()+'/'+dirname+'/'+name + '_Video.mp4', session=session) if flag==0: print('正在下载 \"'+name+'\" 的音频····') BiliBiliDownload(homeurl=homeurl,url=AudioURl, name=os.getcwd()+'/'+dirname+'/'+name+ '_Audio.mp3', session=session) print('正在组合 \"'+name+'\" 的视频和音频····') # CombineVideoAudio(name + '_Video.mp4',name + '_Audio.mp3',name + '_output.mp4') print(' \"'+name+'\" 下载完成！')def BiliBiliDownload(homeurl,url, name, session=requests.session()): headers.update(&#123;'Referer': homeurl&#125;) session.options(url=url, headers=headers,verify=False) # 每次下载1M的数据 begin = 0 end = 1024*512-1 flag=0 while True: headers.update(&#123;'Range': 'bytes='+str(begin) + '-' + str(end)&#125;) res = session.get(url=url, headers=headers,verify=False) if res.status_code != 416: begin = end + 1 end = end + 1024*512 else: headers.update(&#123;'Range': str(end + 1) + '-'&#125;) res = session.get(url=url, headers=headers,verify=False) flag=1 with open(name, 'ab') as fp: fp.write(res.content) fp.flush() # data=data+res.content if flag==1: fp.close() breakdef CombineVideoAudio(videopath,audiopath,outpath): ffmpeg.video.combine_audio(videopath,audiopath,outpath) os.remove(videopath) os.remove(audiopath)if __name__ == '__main__': # av44518113 av = input('请输入视频号：') url='https://www.bilibili.com/video/'+av # 视频选集 range_start=input('从第几集开始？') range_end = input('到第几集结束？') if int(range_start)&lt;=int(range_end): for i in range(int(range_start),int(range_end)+1): GetBiliVideo(url+'?p='+str(i),i-1) else: print('选集不合法！') 运行结果截图： 运行结果 ==微信公众号：== 小术快跑 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '19662-1579695695136-640', name: '小术快跑呀', qrcode: 'https://img-blog.csdnimg.cn/20200111100511753.jpg', keyword: '趁年轻，快快跑~', });","tags":[{"name":"Python","slug":"Python","permalink":"https://www.asyu17.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://www.asyu17.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"bilibili","slug":"bilibili","permalink":"https://www.asyu17.cn/tags/bilibili/"}]},{"title":"【Python爬虫实例学习篇】——3、访问CSDN并编译成exe文件","date":"2020-01-20T02:49:01.000Z","path":"posts/eb3e928b.html","text":"【Python爬虫实例学习篇】——3、访问CSDN并编译成exe文件==首先声明：本博文仅供技术参考，想要提高访问量，好的博文才是王道！！！== 用到的工具: Python 3.6 requests库 request库（为什么会有两个？？？因为我只是单纯想练练手哈哈） random库 time库 pyinstaller库（用于生成exe文件） 目录： 代理库的构建(GetFreeProxies.py) 获取所有文章的链接 访问文章 CSDNVisit.py 编译 结果展示 代理库的构建详情请看【Python爬虫实例学习篇】——2、获取免费IP代理。 获取所有文章的链接思路：访问主页，然后用xpath语法获取所有文章的链接 123456def GetUrlList(homeurl): # 获取所有文章的链接 res = request.urlopen(url=homeurl) html = etree.HTML(res.read().decode('utf-8')) UrlList = html.xpath('//h4//a/@href') return UrlList 访问文章由于CSDN在记录阅读数时会检查IP地址，而又由于免费代理不稳定，可能存在超时等问题，故需要使用try和except来避免错误中断。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869def GetUrlList(homeurl): # 获取所有文章的链接 res = request.urlopen(url=homeurl) html = etree.HTML(res.read().decode('utf-8')) UrlList = html.xpath('//h4//a/@href') return UrlListdef Visit(homeurl, method, UrlList, num, IP_list): # 进行访问 user_agent_list = [ &#123;'Mozilla/5.0(compatible;MSIE9.0;WindowsNT6.1;Trident/5.0)'&#125;, &#123;'Mozilla/4.0(compatible;MSIE8.0;WindowsNT6.0;Trident/4.0)'&#125;, &#123;'Mozilla/4.0(compatible;MSIE7.0;WindowsNT6.0)'&#125;, &#123;'Opera/9.80(WindowsNT6.1;U;en)Presto/2.8.131Version/11.11'&#125;, &#123;'Mozilla/5.0(WindowsNT6.1;rv:2.0.1)Gecko/20100101Firefox/4.0.1'&#125; ] success = 0 fail = 0 i=0 # 第一层while循环用于保证达到目标 while num&gt;success: i=i+1 headers = &#123; 'User-Agent': str(choice(user_agent_list)), 'Referer': homeurl &#125; # 选择代理ip if len(IP_list): ip = IP_list[0].strip('\\n') IP_list.remove(IP_list[0]) else: if method == 1: IP_list = GetFreeProxies.GetFreeProxy() elif method == 2: IP_list = GetFreeProxies.GetFreeProxyList() elif method == 3: IP_list = GetFreeProxies.GetFreeProxyAPI() elif method == 4: IP_list = GetFreeProxies.GetFreeProxyListAPI() else: print(\"暂未设置更多方法！\") return 0 # 第二层for循环用于遍历所有文章 print('-' * 45) print(\"| 序号：\" + str(i)+' |') for j in range(len(UrlList)): print('当前正在浏览第' + str(j) + '篇文章，链接：' + UrlList[j]) url = str(UrlList[j]) proxy_dict = &#123;'https': ip&#125; handler = request.ProxyHandler(proxies=proxy_dict) opener = request.build_opener(handler) req = request.Request(url=url, headers=headers) # try用来避免代理不稳定所导致的报错 try: res = opener.open(req, timeout=10).read().decode('utf-8') html = etree.HTML(res) VisitCount = html.xpath(\"//div/span[@class='read-count']\") sleepnum = random() * 4 + 1 print(str(VisitCount[0].text) + \" || \" + \"随机延迟：\" + \"%2.f\" % sleepnum + \"|| 代理IP为：\" + ip) success=success+1 sleep(sleepnum) except Exception: sleepnum = random() * 2 print('代理IP：' + ip + '连接失败！ ' + '||' + ' 随机延迟：' + \"%2.f\" % sleepnum) fail=fail+1 sleep(sleepnum) break print('成功数：%i' % success + ' 失败数：%i' % fail + ' || 连接数：%i' % (success + fail)+' || 剩余任务数：%i' %(num-success)) CSDNVisit.py说明：博客主页：如 https://blog.csdn.net/qq_40528553阅读次数：如 5000代理方法： 如 2 方法1，代表直接访问网页获取1个代理ip 方法2，代表直接访问网页获取15个代理ip，推荐使用，成功率超50% 方法3，代表使用API一次获取1个代理ip 方法4，代表使用API一次获取15个代理ip 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110from urllib import requestfrom random import choice, randomfrom time import sleepfrom lxml import etreeimport GetFreeProxiesdef VisitCsdn(homeurl, num=1, method=4): # method=1,从网页上获取一个免费代理 # method=2,从网页上获取一列免费代理 # method=3,从API上获取一个免费代理 # method=4,从API上获取一列免费代理，推荐使用API获取代理 if method == 1: IP_list = GetFreeProxies.GetFreeProxy() elif method == 2: IP_list = GetFreeProxies.GetFreeProxyList() elif method == 3: IP_list = GetFreeProxies.GetFreeProxyAPI() elif method == 4: IP_list = GetFreeProxies.GetFreeProxyListAPI() else: print(\"暂未设置更多方法！\") return 0 UrlList = GetUrlList(homeurl) Visit(homeurl,method, UrlList, num, IP_list)def GetUrlList(homeurl): # 获取所有文章的链接 res = request.urlopen(url=homeurl) html = etree.HTML(res.read().decode('utf-8')) UrlList = html.xpath('//h4//a/@href') return UrlListdef Visit(homeurl, method, UrlList, num, IP_list): # 进行访问 user_agent_list = [ &#123;'Mozilla/5.0(compatible;MSIE9.0;WindowsNT6.1;Trident/5.0)'&#125;, &#123;'Mozilla/4.0(compatible;MSIE8.0;WindowsNT6.0;Trident/4.0)'&#125;, &#123;'Mozilla/4.0(compatible;MSIE7.0;WindowsNT6.0)'&#125;, &#123;'Opera/9.80(WindowsNT6.1;U;en)Presto/2.8.131Version/11.11'&#125;, &#123;'Mozilla/5.0(WindowsNT6.1;rv:2.0.1)Gecko/20100101Firefox/4.0.1'&#125; ] success = 0 fail = 0 i=0 # 第一层while循环用于保证达到目标 while num&gt;success: i=i+1 headers = &#123; 'User-Agent': str(choice(user_agent_list)), 'Referer': homeurl &#125; # 选择代理ip if len(IP_list): ip = IP_list[0].strip('\\n') IP_list.remove(IP_list[0]) else: if method == 1: IP_list = GetFreeProxies.GetFreeProxy() elif method == 2: IP_list = GetFreeProxies.GetFreeProxyList() elif method == 3: IP_list = GetFreeProxies.GetFreeProxyAPI() elif method == 4: IP_list = GetFreeProxies.GetFreeProxyListAPI() else: print(\"暂未设置更多方法！\") return 0 # 第二层for循环用于遍历所有文章 print('-' * 45) print(\"| 序号：\" + str(i)+' |') for j in range(len(UrlList)): print('当前正在浏览第' + str(j) + '篇文章，链接：' + UrlList[j]) url = str(UrlList[j]) proxy_dict = &#123;'https': ip&#125; handler = request.ProxyHandler(proxies=proxy_dict) opener = request.build_opener(handler) req = request.Request(url=url, headers=headers) # try用来避免代理不稳定所导致的报错 try: res = opener.open(req, timeout=10).read().decode('utf-8') html = etree.HTML(res) VisitCount = html.xpath(\"//div/span[@class='read-count']\") sleepnum = random() * 4 + 1 print(str(VisitCount[0].text) + \" || \" + \"随机延迟：\" + \"%2.f\" % sleepnum + \"|| 代理IP为：\" + ip) success=success+1 sleep(sleepnum) except Exception: sleepnum = random() * 2 print('代理IP：' + ip + '连接失败！ ' + '||' + ' 随机延迟：' + \"%2.f\" % sleepnum) fail=fail+1 sleep(sleepnum) break print('成功数：%i' % success + ' 失败数：%i' % fail + ' || 连接数：%i' % (success + fail)+' || 剩余任务数：%i' %(num-success))if __name__ == '__main__': print('*'*45) print('* CSDN刷阅读量工具正式版 V1.0\\t *') print('* -by asyu17\\t *') print('* 2020/1/14\\t *') print('*'*45) homeurl = input('请输入博客主页：') num = input('请输入阅读次数：') method = input('请选择代理方法：') VisitCsdn(homeurl, int(num), method=int(method)) 编译 pip install pyinstaller 安装pyinstaller库 在cmd中定位到python脚本所在目录 如 cd D:\\code\\python\\2020_spider_1 随后执行 pyinstaller -F -i asyu17.ico CSDNVisit.py 即可 可执行文件在dist目录中 在这里插入图片描述 6、结果展示： 结果展示 ==微信公众号：== 小术快跑 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '19662-1579695695136-640', name: '小术快跑呀', qrcode: 'https://img-blog.csdnimg.cn/20200111100511753.jpg', keyword: '趁年轻，快快跑~', });","tags":[{"name":"Python","slug":"Python","permalink":"https://www.asyu17.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://www.asyu17.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"PyQt5","slug":"PyQt5","permalink":"https://www.asyu17.cn/tags/PyQt5/"},{"name":"CSDN","slug":"CSDN","permalink":"https://www.asyu17.cn/tags/CSDN/"}]},{"title":"【Python爬虫实例学习篇】——2、获取免费IP代理","date":"2020-01-12T12:21:01.000Z","path":"posts/25a81709.html","text":"【Python爬虫实例学习篇】——2、获取免费IP代理由于在使用爬虫时经常会检查IP地址，因此有必要找到一个获取IP代理的地方。经过苦苦寻找，终于找到了一个质量还算过得去同时提供API接口的免费代理网站，下面是API的封装过程。 使用工具1.Python 3.62.requests库3.免费代理网站 获取一个免费代理该免费代理网站提供了两个，一个是提供一个免费代理，另一个是提供一页免费代理（一页最多15个）。 1234567891011121314151617import requestsdef GetFreeProxy(): # 获取一个免费代理 url='https://www.freeip.top/api/proxy_ip' ip=list(range(1)) try: res=requests.get(url=url,timeout=20) # 将返回数据进行json解析 result = res.json() ip[0]=result['data']['ip']+':'+result['data']['port'] return ip except Exception: print('获取代理ip失败！正在重试···') # 异常重调 GetFreeProxy() return 0 获取一页免费代理12345678910111213141516171819202122def GetFreeProxyList(page=1,country='all',isp='None',rder_by='validated_at',order_rule='DESC'):# 获取一个免费代理列表#(以下参数是开发文档上提供的，但并未提供提交方法，就先放这里了)# 参数名 数据类型 必传 说明 例子# page int N 第几页 1# country string N 所属国 中国,美国# isp string N ISP 电信,阿里云# order_by string N 排序字段 speed:响应速度,validated_at:最新校验时间 created_at:存活时间# order_rule string N 排序方向 DESC:降序 ASC:升序 url='https://www.freeip.top/api/proxy_ips' try: res = requests.get(url=url,timeout=20) result = res.json() # 将返回数据进行json解析 ip=list(range(int(result['data']['to']))) for i in range(int(result['data']['to'])): ip[i]=result['data']['data'][i]['ip'] + ':' + result['data']['data'][i]['port'] return ip except Exception: print('获取代理ip列表失败！正在重试···') GetFreeProxyList() return 0 结果展示获取一个免费代理： 获取一个免费代理 获取一页免费代理： 获取一页免费代理 ==微信公众号：== 小术快跑","tags":[{"name":"Python","slug":"Python","permalink":"https://www.asyu17.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://www.asyu17.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"proxy","slug":"proxy","permalink":"https://www.asyu17.cn/tags/proxy/"},{"name":"代理","slug":"代理","permalink":"https://www.asyu17.cn/tags/%E4%BB%A3%E7%90%86/"}]},{"title":"【Python爬虫实例学习篇】——1、获取拉勾网职位信息","date":"2020-01-10T13:53:01.000Z","path":"posts/9f966ca6.html","text":"【Python爬虫实例学习篇】——1、获取拉勾网职位信息 毕业季就要到了，打算上拉钩网爬一下有关实习岗位的招聘信息。刚写完几行代码进行调试发现一直提示：{“status”:false,”msg”== :”您操作太频繁,请稍后再访问”,”clientIp”:”223.155.85.177”,”state”:2402}，此时进入网页一看，能够正常进行访问，并没有出现上述提示语，据此判断存在反爬虫机制。经过一番尝试发现是cookie的问题，下面是解决问题的详细过程。 问题一开始想用urllib库来获取招聘信息结果发现返回结果一直是操作频繁，代码如下： 123456789101112131415161718from urllib import request,parseKeyWord=\"python\"url=\"https://www.lagou.com/jobs/list_\"+KeyWord+\"?&amp;cl=false&amp;fromSearch=true&amp;labelWords=&amp;suginput=\"url_GetJob=\"https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false\"headers=&#123; \"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36\", \"Referer\":url&#125;data=&#123; \"first\":\"true\", \"pn\":\"1\", \"kd\":KeyWord&#125;req=request.Request(url_GetJob,headers=headers,data=parse.urlencode(data).encode('utf-8'))response=request.urlopen(req)print(response.read().decode('utf-8')) 返回结果为： 错误信息 此时网页直接访问情况： 网页访问状况 解决办法方法1：利用http.cookiejar12345678910111213141516171819202122232425from urllib import request, parseimport http.cookiejarKeyWord = \"python\"url = \"https://www.lagou.com/jobs/list_\" + KeyWord + \"?&amp;cl=false&amp;fromSearch=true&amp;labelWords=&amp;suginput=\"url_GetJob = \"https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false\"headers = &#123; \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36\", \"Referer\": url&#125;data = &#123; \"first\": \"true\", \"pn\": \"1\", \"kd\": KeyWord&#125;cookie_jar = http.cookiejar.CookieJar()handler = request.HTTPCookieProcessor(cookie_jar)opener = request.build_opener(handler)req = request.Request(url, headers=headers)opener.open(req) # 目的是获取Cookiereq2=request.Request(url_GetJob,headers=headers, data=parse.urlencode(data).encode('utf-8'))res = opener.open(req2)print(res.read().decode('utf-8')) 方法2：利用requests.session1234567891011121314151617181920import requestsKeyWord = \"python\"url = \"https://www.lagou.com/jobs/list_\" + KeyWord + \"?&amp;cl=false&amp;fromSearch=true&amp;labelWords=&amp;suginput=\"url_GetJob = \"https://www.lagou.com/jobs/positionAjax.json?needAddtionalResult=false\"headers = &#123; \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3970.5 Safari/537.36\", \"Referer\": url&#125;# 创建会话session = requests.session()res1 = session.get(url, headers=headers, verify=False)# 保持会话提交表单data = &#123; \"first\": \"true\", \"pn\": \"1\", \"kd\": KeyWord&#125;res = session.post(url_GetJob, headers=headers, data=data, verify=False) 结果 正确返回结果 ==微信公众号：== 小术快跑 const btw = new BTWPlugin(); btw.init({ id: 'container', blogId: '19662-1579695695136-640', name: '小术快跑呀', qrcode: 'https://img-blog.csdnimg.cn/20200111100511753.jpg', keyword: '趁年轻，快快跑~', });","tags":[{"name":"Python","slug":"Python","permalink":"https://www.asyu17.cn/tags/Python/"},{"name":"爬虫","slug":"爬虫","permalink":"https://www.asyu17.cn/tags/%E7%88%AC%E8%99%AB/"},{"name":"拉勾网","slug":"拉勾网","permalink":"https://www.asyu17.cn/tags/%E6%8B%89%E5%8B%BE%E7%BD%91/"}]}]